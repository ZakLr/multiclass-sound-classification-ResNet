{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the code for the solution proposed by THE UNCLES team concering the sound classification challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The given folders structure:\n",
    "\n",
    " - train:\n",
    " - 10 subfolders that contain soundfiles of the sound classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The soundfiles are of a short format and the number of soundfiles in every class is nearly the same in every subfolder, the classes are balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to produce better results, we will use some data augmentation techniques too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will process audio files into **Mel Spectrograms**, a visual representation of sound.\n",
    "- The model will be based on **ResNet-18** not pretrained.\n",
    "- We implement **Mixup augmentation**, a data augmentation technique to improve generalization.\n",
    "- We also use **early stopping and learning rate decay** to optimize training.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I-Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import torch # for dl\n",
    "import torchaudio # for audio processing\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split # for handling data loading and splitting\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB, TimeMasking, FrequencyMasking # for audio processing \n",
    "from sklearn.metrics import f1_score # the used metric in the challenge\n",
    "from tqdm import tqdm # for progress bar visualization\n",
    "import torchvision.models as models # for using model architectures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II-Configuration setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the key **hyperparameters** for training:\n",
    "\n",
    "- `sample_rate`: The number of samples per second in an audio clip.\n",
    "- `n_mels`: Number of mel-frequency bins in spectrograms.\n",
    "- `batch_size`: Number of samples per batch for training.\n",
    "- `epochs`: Total training cycles.\n",
    "- `learning_rate`: Step size for optimizer updates.\n",
    "- `train_split`: Proportion of data used for training.\n",
    "- `patience`: How long to wait before early stopping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"sample_rate\": 16000,\n",
    "    \"n_mels\": 128,\n",
    "    \"n_fft\": 1024,\n",
    "    \"hop_length\": 512,\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 60,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"audio_duration\": 2,\n",
    "    \"num_classes\": 10,\n",
    "    \"train_split\": 0.8,\n",
    "    \"patience\": 10,\n",
    "    \"lr_decay_factor\": 0.5,\n",
    "    \"lr_decay_patience\": 2,\n",
    "    \"weight_decay\": 5e-4,\n",
    "    \"mixup_alpha\": 0.4  # Mixup hyperparameter (higher means stronger mixup)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iii-Defining the AudioDataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, train=True):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.train = train\n",
    "        \n",
    "        self.mel_spec_transform = MelSpectrogram(\n",
    "            sample_rate=CONFIG[\"sample_rate\"],\n",
    "            n_fft=CONFIG[\"n_fft\"],\n",
    "            hop_length=CONFIG[\"hop_length\"],\n",
    "            n_mels=CONFIG[\"n_mels\"]\n",
    "        )\n",
    "        self.amp_to_db = AmplitudeToDB(top_db=80)\n",
    "\n",
    "        # Spectrogram augmentations (SpecAugment)\n",
    "        self.time_mask = TimeMasking(time_mask_param=20)\n",
    "        self.freq_mask = FrequencyMasking(freq_mask_param=10)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.file_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "        if sr != CONFIG[\"sample_rate\"]:\n",
    "            waveform = torchaudio.transforms.Resample(sr, CONFIG[\"sample_rate\"])(waveform)\n",
    "\n",
    "        # Ensure consistent length\n",
    "        target_length = int(CONFIG[\"audio_duration\"] * CONFIG[\"sample_rate\"])\n",
    "        if waveform.size(1) > target_length:\n",
    "            start = np.random.randint(0, waveform.size(1) - target_length) if self.train else 0\n",
    "            waveform = waveform[:, start:start + target_length]\n",
    "        else:\n",
    "            waveform = F.pad(waveform, (0, target_length - waveform.size(1)))\n",
    "        \n",
    "        # ----- Waveform Augmentations -----\n",
    "        if self.train:\n",
    "            waveform = torch.roll(waveform, shifts=np.random.randint(-1600, 1600), dims=1)\n",
    "            waveform = waveform * np.random.uniform(0.8, 1.2)\n",
    "        # -----------------------------------\n",
    "\n",
    "        mel_spec = self.mel_spec_transform(waveform)\n",
    "        mel_spec = self.amp_to_db(mel_spec)\n",
    "        mel_spec = (mel_spec - mel_spec.mean()) / (mel_spec.std() + 1e-8)  # Normalize\n",
    "        \n",
    "        if self.train:\n",
    "            mel_spec = self.time_mask(mel_spec)\n",
    "            mel_spec = self.freq_mask(mel_spec)\n",
    "\n",
    "        return mel_spec, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mixup** is a data augmentation technique where two training samples are mixed together using a random weight **λ**.\n",
    "\n",
    "- It helps improve model generalization.\n",
    "- It prevents the model from becoming overconfident.\n",
    "- Instead of training on a single label, the model learns a weighted combination of labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixup augmentation\n",
    "def mixup_data(x, y, alpha=CONFIG[\"mixup_alpha\"]):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iv-Function to display a spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram(file_path):\n",
    "    waveform, sr = torchaudio.load(file_path)\n",
    "    mel_spec_transform = MelSpectrogram(\n",
    "        sample_rate=CONFIG[\"sample_rate\"],\n",
    "        n_fft=CONFIG[\"n_fft\"],\n",
    "        hop_length=CONFIG[\"hop_length\"],\n",
    "        n_mels=CONFIG[\"n_mels\"]\n",
    "    )\n",
    "    mel_spec = mel_spec_transform(waveform)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(mel_spec.log2()[0].numpy(), cmap=\"inferno\", aspect=\"auto\")\n",
    "    plt.title(\"Mel Spectrogram\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v-Defining the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientResNetAudio(nn.Module):\n",
    "    def __init__(self, num_classes=10, input_channels=1):\n",
    "        super(EfficientResNetAudio, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=False)\n",
    "        self.resnet.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        in_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vii-Defining how the training epoch should be like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss, all_preds, all_labels = 0, [], []\n",
    "    \n",
    "    for data, target in tqdm(train_loader, desc=\"Training\"):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Apply Mixup augmentation\n",
    "        mixed_data, targets_a, targets_b, lam = mixup_data(data, target, CONFIG[\"mixup_alpha\"])\n",
    "        output = model(mixed_data)\n",
    "        \n",
    "        loss = mixup_criterion(criterion, output, targets_a, targets_b, lam)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        all_preds.extend(output.argmax(dim=1).cpu().numpy())\n",
    "        all_labels.extend(target.cpu().numpy())\n",
    "\n",
    "    return total_loss / len(train_loader), f1_score(all_labels, all_preds, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iix-Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "data_dir = \"/kaggle/input/urban-sound-classification/train\"\n",
    "file_paths, labels = [], []\n",
    "\n",
    "for label_idx, class_name in enumerate(sorted(os.listdir(data_dir))):\n",
    "    class_dir = os.path.join(data_dir, class_name)\n",
    "    for file_name in os.listdir(class_dir):\n",
    "        file_paths.append(os.path.join(class_dir, file_name))\n",
    "        labels.append(label_idx)\n",
    "\n",
    "dataset = AudioDataset(file_paths, labels, train=True)\n",
    "train_size = int(CONFIG[\"train_split\"] * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "model = EfficientResNetAudio(num_classes=CONFIG[\"num_classes\"]).to(device)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"], weight_decay=CONFIG[\"weight_decay\"])\n",
    "\n",
    "for epoch in range(CONFIG[\"epochs\"]):\n",
    "    train_epoch(model, train_loader, criterion, optimizer, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ix-Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "def test(model, test_loader, device):\n",
    "    total_loss, all_preds, all_labels = 0, [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            all_preds.extend(output.argmax(dim=1).cpu().numpy())\n",
    "            all_labels.extend(target.cpu().numpy())\n",
    "\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    print(f\"Test F1 Score: {f1:.4f}\")\n",
    "\n",
    "test(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script for building the submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchaudio.transforms import MelSpectrogram\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Config\n",
    "CONFIG = {\n",
    "    \"sample_rate\": 16000,\n",
    "    \"n_mels\": 128,\n",
    "    \"n_fft\": 1024,\n",
    "    \"hop_length\": 512,\n",
    "    \"batch_size\": 32,\n",
    "    \"model_path\": \"best_model.pth\",  # Update based on the best saved model\n",
    "}\n",
    "\n",
    "# Class Labels\n",
    "CLASSES = [\n",
    "    \"airport\", \"bus\", \"metro\", \"metro_station\", \"park\",\n",
    "    \"public_square\", \"shopping_mall\", \"street_pedestrian\",\n",
    "    \"street_traffic\", \"tram\"\n",
    "]\n",
    "class_to_idx = {i: label for i, label in enumerate(CLASSES)}\n",
    "\n",
    "# ResNet Model for Audio\n",
    "class ResNetAudio(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNetAudio, self).__init__()\n",
    "        self.resnet = models.resnet18(weights=None)\n",
    "        self.resnet.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Load Model\n",
    "model = ResNetAudio(num_classes=10).to(device)\n",
    "model.load_state_dict(torch.load(CONFIG[\"model_path\"], map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Function to process test audio file\n",
    "def process_audio(filepath):\n",
    "    waveform, sr = torchaudio.load(filepath)\n",
    "\n",
    "    if sr != CONFIG[\"sample_rate\"]:\n",
    "        waveform = torchaudio.transforms.Resample(sr, CONFIG[\"sample_rate\"])(waveform)\n",
    "\n",
    "    mel_spec = MelSpectrogram(\n",
    "        sample_rate=CONFIG[\"sample_rate\"],\n",
    "        n_fft=CONFIG[\"n_fft\"],\n",
    "        hop_length=CONFIG[\"hop_length\"],\n",
    "        n_mels=CONFIG[\"n_mels\"]\n",
    "    )(waveform)\n",
    "\n",
    "    mel_spec = torch.log(mel_spec + 1e-9)\n",
    "    mel_spec = mel_spec.expand(3, -1, -1)  # Convert to 3-channel for ResNet\n",
    "\n",
    "    return mel_spec\n",
    "\n",
    "# Predict labels for test data\n",
    "test_dir = \"/kaggle/input/urban-sound-classification/test\"\n",
    "test_files = sorted(os.listdir(test_dir))  # Ensure filenames are sorted for consistency\n",
    "predictions = []\n",
    "\n",
    "for filename in test_files:\n",
    "    filepath = os.path.join(test_dir, filename)\n",
    "    mel_spec = process_audio(filepath)\n",
    "    mel_spec = mel_spec.unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(mel_spec)\n",
    "        pred_label = torch.argmax(output, dim=1).item()\n",
    "\n",
    "    predictions.append([filename, class_to_idx[pred_label]])\n",
    "\n",
    "# Save predictions to CSV\n",
    "submission_df = pd.DataFrame(predictions, columns=[\"filename\", \"scene_label\"])\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"✅ Predictions saved to submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
